{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080675c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn,optim\n",
    "path = r\"C:\\Users\\kanna\\Desktop\\GUVI\\4.Data\\Breast Cancer\\train\"\n",
    "transformation = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((640,640)),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")# Line5 to 9 (lets resize the image and convert it to tensor(numerical value of pixel representation))\n",
    "data = datasets.ImageFolder(path,transform=transformation) # Loading the data and converting to tensor\n",
    "# data.classes\n",
    "num_classes = len(data.classes) # finding the number of classes, not using now, but needed in case of multi class classifiation.\n",
    "\n",
    "loader = DataLoader(data, batch_size=64,shuffle=True) #convert  the entire dataset in batches of size 64 each, in 1 batch we have 64 images and 64 labels.\n",
    "\n",
    "class bcnn(nn.Module):\n",
    "    def __init__(self,input_channels):\n",
    "        super(bcnn,self).__init__()\n",
    "        self.cv1 = nn.Conv2d(input_channels,out_channels=32,kernel_size=3)# 32 = channels, 3 = Kernel, Default stride = 1\n",
    "                                                 # 640+0-3+1 = 638*638\n",
    "                                                 # kernel will be constructed by the model, no need to mention.\n",
    "        self.p1 = nn.MaxPool2d(kernel_size=3,stride=3) # 3 = Kernel size, stride = 3\n",
    "                                    # 638-3/3+1 --> 212*212*32 pixels\n",
    "                                    # reduce the number of pixels and maintain only the important features\n",
    "                                    # used for computation efficiency\n",
    "        self.flatten = nn.Flatten() # transforms into single dimension, 2 dimension(tabular data) into single dimension\n",
    "        self.fc1 = nn.Linear(212*212*32,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,1) # 1 at the last layer because of binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.cv1(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid (self.fc3(x))\n",
    "        return x \n",
    "img, label = data[0]\n",
    "input_channels = img.shape[0]\n",
    "model = bcnn(input_channels) # object of the neural network\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "num_epoch = 10\n",
    "for epoch in range(1,num_epoch+1):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for x,y in loader:\n",
    "        y = y.float().unsqueeze(1) # to create as list of list\n",
    "        optimizer.zero_grad() # resetting the gradiants\n",
    "        output = model(x) # forward function is applied\n",
    "        loss = criterion(output,y) # loss in output layer\n",
    "        loss.backward() # compute error for each neuron in previous layers\n",
    "        optimizer.step() # update weights\n",
    "        total_loss = total_loss+loss.item()\n",
    "    print (f\"{epoch}/{num_epoch}: Loss --> {total_loss}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
